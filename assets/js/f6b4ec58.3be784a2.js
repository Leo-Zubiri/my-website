"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[5842],{3905:(e,a,t)=>{t.d(a,{Zo:()=>m,kt:()=>f});var n=t(67294);function r(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function o(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}function i(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?o(Object(t),!0).forEach((function(a){r(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function s(e,a){if(null==e)return{};var t,n,r=function(e,a){if(null==e)return{};var t,n,r={},o=Object.keys(e);for(n=0;n<o.length;n++)t=o[n],a.indexOf(t)>=0||(r[t]=e[t]);return r}(e,a);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)t=o[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var l=n.createContext({}),d=function(e){var a=n.useContext(l),t=a;return e&&(t="function"==typeof e?e(a):i(i({},a),e)),t},m=function(e){var a=d(e.components);return n.createElement(l.Provider,{value:a},e.children)},p="mdxType",c={inlineCode:"code",wrapper:function(e){var a=e.children;return n.createElement(n.Fragment,{},a)}},u=n.forwardRef((function(e,a){var t=e.components,r=e.mdxType,o=e.originalType,l=e.parentName,m=s(e,["components","mdxType","originalType","parentName"]),p=d(t),u=r,f=p["".concat(l,".").concat(u)]||p[u]||c[u]||o;return t?n.createElement(f,i(i({ref:a},m),{},{components:t})):n.createElement(f,i({ref:a},m))}));function f(e,a){var t=arguments,r=a&&a.mdxType;if("string"==typeof e||r){var o=t.length,i=new Array(o);i[0]=u;var s={};for(var l in a)hasOwnProperty.call(a,l)&&(s[l]=a[l]);s.originalType=e,s[p]="string"==typeof e?e:r,i[1]=s;for(var d=2;d<o;d++)i[d]=t[d];return n.createElement.apply(null,i)}return n.createElement.apply(null,t)}u.displayName="MDXCreateElement"},82348:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>i,default:()=>c,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var n=t(87462),r=(t(67294),t(3905));const o={},i="Machine Learning",s={unversionedId:"datascience/kaggle/ML",id:"datascience/kaggle/ML",title:"Machine Learning",description:"El machine learning, o aprendizaje autom\xe1tico, es un campo de la inteligencia artificial (IA) que permite a las computadoras aprender de los datos y mejorar con la experiencia sin ser programadas expl\xedcitamente.",source:"@site/docs/datascience/kaggle/01_ML.md",sourceDirName:"datascience/kaggle",slug:"/datascience/kaggle/ML",permalink:"/my-website/docs/datascience/kaggle/ML",draft:!1,editUrl:"https://github.com/Leo-Zubiri/my-website/tree/master/docs/datascience/kaggle/01_ML.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Methodology",permalink:"/my-website/docs/datascience/IBM/methodology"},next:{title:"Pandas",permalink:"/my-website/docs/datascience/kaggle/pandas"}},l={},d=[{value:"How Models Work",id:"how-models-work",level:2},{value:"Building a model",id:"building-a-model",level:3},{value:"Model Validation",id:"model-validation",level:3},{value:"Underfitting and Overfitting",id:"underfitting-and-overfitting",level:3},{value:"Example",id:"example",level:4},{value:"Random Forests",id:"random-forests",level:3}],m={toc:d},p="wrapper";function c(e){let{components:a,...o}=e;return(0,r.kt)(p,(0,n.Z)({},m,o,{components:a,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"machine-learning"},"Machine Learning"),(0,r.kt)("p",null,"El machine learning, o aprendizaje autom\xe1tico, es un campo de la inteligencia artificial (IA) que permite a las computadoras aprender de los datos y mejorar con la experiencia sin ser programadas expl\xedcitamente. "),(0,r.kt)("p",null,"El machine learning se basa en el desarrollo de algoritmos y modelos estad\xedsticos que permiten a las computadoras realizar tareas bas\xe1ndose en patrones e inferencias. Los modelos aprenden utilizando m\xe9todos iterativos para ajustar los par\xe1metros y mejorar su rendimiento con el tiempo. "),(0,r.kt)("p",null,"El machine learning se utiliza en una gran variedad de \xe1mbitos, como: Reconocimiento facial, Detecci\xf3n de spam, Reconocimiento de voz, Segmentaci\xf3n de clientes. "),(0,r.kt)("h2",{id:"how-models-work"},"How Models Work"),(0,r.kt)("p",null,"Considerando el siguiente caso:"),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"Su primo ha hecho millones de d\xf3lares especulando en bienes ra\xedces. Se ha ofrecido a convertirse en socios de negocios contigo por tu inter\xe9s en la ciencia de datos. Suministrar\xe1 el dinero, y suministrar\xe1s modelos que predicen cu\xe1nto valen varias casas. Le preguntas a tu primo c\xf3mo ha predicho valores inmobiliarios en el pasado, y dice que es s\xf3lo una intuici\xf3n. Pero m\xe1s cuestionamientos revelan que ha identificado patrones de precios de casas que ha visto en el pasado, y usa esos patrones para hacer predicciones para nuevas casas que est\xe1 considerando.")),(0,r.kt)("p",null,"Para comenzar con un modelo simple se puede utilizar un arbol de decision."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Decision tree.")," Un \xe1rbol de decisiones es una herramienta visual que se utiliza para tomar decisiones de manera estructurada, al representar los posibles resultados y consecuencias de una serie de decisiones. Se le llama as\xed por su similitud con un \xe1rbol con muchas ramas."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"fitting or training the model"),". Se refiere a la captura de datos para capturar patrones. Los datos utilizados para esto, reciben el nombre de ",(0,r.kt)("strong",{parentName:"p"},"training data")),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"El siguiente es un ejemplo de un arbol de decision, se recorre el arbol segun las caracteristicas y se obtiene el resultado hasta llegar a la parte final del arbol. El punto final de la rama es conocida como hoja (",(0,r.kt)("strong",{parentName:"p"},"leaf"),")"),(0,r.kt)("p",{parentName:"blockquote"},(0,r.kt)("img",{src:t(21684).Z,width:"709",height:"449"}))),(0,r.kt)("p",null,"El primer paso es familiarizarse con los datos, para esto en python existe una libreria llamada Pandas para explorar y manipular los datos. El tipo de datos ",(0,r.kt)("strong",{parentName:"p"},"dataframe")," de esta libreria representa los datos como una tabla. Es similar al relacionarlo con una hoja en excel o una tabla dentro de una base de datos SQL. "),(0,r.kt)("hr",null),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Utilizando pandas")),(0,r.kt)("p",null,"Uso de ",(0,r.kt)("strong",{parentName:"p"},"describe")," para mostrar informacion general sobre determinados datos leidos de un conjunto de datos "),(0,r.kt)("p",null,(0,r.kt)("a",{target:"_blank",href:t(83924).Z},(0,r.kt)("strong",null,"Descarga dataset melb_data.csv"))),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},"import pandas as pd \n\n# save filepath to variable for easier access\nmelbourne_file_path = './01_basic_data_exploration/melb_data.csv'\n\n# read the data and store data in DataFrame titled melbourne_data\nmelbourne_data = pd.read_csv(melbourne_file_path) \n\n# print a summary of the data in Melbourne data\nmelbourne_data.describe()\nprint(melbourne_data.describe())\n\n# print dataset columns\nprint(melbourne_data.columns)\n\n# drop not available values in the dataset  axis 0 = rows  1 = columns\nmelbourne_data = melbourne_data.dropna(axis=0)\n\n# Extract a column from the dataframe\ny = melbourne_data.Price\n\n# Extract features from dataframe (its like a subset)\nmelbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'Lattitude', 'Longtitude']\nX = melbourne_data[melbourne_features]\nprint(X.describe())\n\n")),(0,r.kt)("p",null,(0,r.kt)("img",{src:t(85704).Z,width:"2167",height:"334"})),(0,r.kt)("p",null,(0,r.kt)("img",{src:t(56225).Z,width:"870",height:"225"})),(0,r.kt)("h3",{id:"building-a-model"},"Building a model"),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},(0,r.kt)("strong",{parentName:"p"},"scikit-learn")," its a library to create models using the types of data stored in dataframes."),(0,r.kt)("p",{parentName:"blockquote"},(0,r.kt)("strong",{parentName:"p"},"The steps to building models")),(0,r.kt)("ol",{parentName:"blockquote"},(0,r.kt)("li",{parentName:"ol"},"Define. The type of model"),(0,r.kt)("li",{parentName:"ol"},"Fit. Capture patterns from provided data"),(0,r.kt)("li",{parentName:"ol"},"Predict. "),(0,r.kt)("li",{parentName:"ol"},"Evaluate. Accurate of the model predictions."))),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Decision tree with scikit-learn")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},"from sklearn.tree import DecisionTreeRegressor\nimport pandas as pd \n\n# save filepath to variable for easier access\nmelbourne_file_path = './01_basic_data_exploration/melb_data.csv'\n\n# read the data and store data in DataFrame titled melbourne_data\nmelbourne_data = pd.read_csv(melbourne_file_path) \n\n# Extract a column to predict\nY = melbourne_data.Price\n\n# Extract features from dataframe (its like a subset)\nmelbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'Lattitude', 'Longtitude']\nX = melbourne_data[melbourne_features]\n\n# Define model. Specify a number for random_state to ensure same results each run\nmelbourne_model = DecisionTreeRegressor(random_state=1)\n\n# Fit model\nmelbourne_model.fit(X, Y)\n\n# Predict\nprint(\"Making predictions for the following 5 houses:\")\nprint(X.head())\nprint(\"The predictions are\")\nprint(melbourne_model.predict(X.head()))\n")),(0,r.kt)("p",null,(0,r.kt)("img",{src:t(25277).Z,width:"739",height:"240"})),(0,r.kt)("hr",null),(0,r.kt)("h3",{id:"model-validation"},"Model Validation"),(0,r.kt)("p",null,"Measure the quality of your model. Measuring model quality is the key to iteratively improving your models."),(0,r.kt)("p",null,"There are many metrics for summarizing model quality. ",(0,r.kt)("strong",{parentName:"p"},"Mean Absolute Error (also called MAE)"),":"),(0,r.kt)("p",null,"The prediction error for each house is: "),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"error=actual\u2212predicted\n")),(0,r.kt)("p",null,"So, if a house cost $150,000 and you predicted it would cost $100,000 the error is $50,000."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},"from sklearn.metrics import mean_absolute_error\n\npredicted_home_prices = melbourne_model.predict(X)\nmean_absolute_error(y, predicted_home_prices)\n")),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"Es buena practica dividir la data en dos partes, una que servira para la creacion del modelo y la otra para someterlo a pruebas. Existen funciones para conseguir este comportamiento ",(0,r.kt)("strong",{parentName:"p"},"train_test_split"))),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from sklearn.model_selection import train_test_split\n\n# split data into training and validation data, for both features and target\n# The split is based on a random number generator. Supplying a numeric value to\n# the random_state argument guarantees we get the same split every time we\n# run this script.\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)\n# Define model\nmelbourne_model = DecisionTreeRegressor()\n# Fit model\nmelbourne_model.fit(train_X, train_y)\n\n# get predicted prices on validation data\nval_predictions = melbourne_model.predict(val_X)\nprint(mean_absolute_error(val_y, val_predictions))\n")),(0,r.kt)("h3",{id:"underfitting-and-overfitting"},"Underfitting and Overfitting"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Overfitting"),". When a model matches the training data almost perfectly, but does poorly in validation and other new data"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Underfitting"),". When a model fails to capture important distinctions and patterns in the data, so it performs poorly even in training data")),(0,r.kt)("p",null,"In the case of a decission tree, as the tree gets deeper, the dataset gets sliced up into leaves with fewer houses. If a tree only had 1 split, it divides the data into 2 groups. If each group is split again, we would get 4 groups of houses."),(0,r.kt)("p",null,"When we divide the houses amongst many leaves, we also have fewer houses in each leaf. Leaves with very few houses will make predictions that are quite close to those homes' actual values, but they may make very unreliable predictions for new data (because each prediction is based on only a few houses)."),(0,r.kt)("h4",{id:"example"},"Example"),(0,r.kt)("p",null,"The max_leaf_nodes argument provides a very sensible way to control overfitting vs underfitting. We can use a utility function to help compare MAE scores from different values for max_leaf_nodes"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},"from sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\n\ndef get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)\n")),(0,r.kt)("p",null,"We can use a for-loop to compare the accuracy of models built with different values for max_leaf_nodes."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},'# compare MAE with differing values of max_leaf_nodes\nfor max_leaf_nodes in [5, 50, 500, 5000]:\n    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print("Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d" %(max_leaf_nodes, my_mae))\n')),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Of the options listed, 500 is the optimal number of leaves.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"Max leaf nodes: 5            Mean Absolute Error:  347380\nMax leaf nodes: 50           Mean Absolute Error:  258171\nMax leaf nodes: 500          Mean Absolute Error:  243495\nMax leaf nodes: 5000         Mean Absolute Error:  254983\n")),(0,r.kt)("h3",{id:"random-forests"},"Random Forests"),(0,r.kt)("p",null,"The random forest uses many trees, and it makes a prediction by averaging the predictions of each component tree. It generally has much better predictive accuracy than a single decision tree and it works well with default parameters"),(0,r.kt)("p",null,"We build a random forest model similarly to how we built a decision tree in scikit-learn - this time using the ",(0,r.kt)("strong",{parentName:"p"},"RandomForestRegressor")," class instead of DecisionTreeRegressor."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\nforest_model = RandomForestRegressor(random_state=1)\nforest_model.fit(train_X, train_y)\nmelb_preds = forest_model.predict(val_X)\nprint(mean_absolute_error(val_y, melb_preds))\n")))}c.isMDXComponent=!0},83924:(e,a,t)=>{t.d(a,{Z:()=>n});const n=t.p+"assets/files/melb_data-5f83ebd28c2d1e67b57100a37695a2c6.csv"},21684:(e,a,t)=>{t.d(a,{Z:()=>n});const n=t.p+"assets/images/ML_decisiontree1-6a8207653ebf915d1101689f718cfee6.jpg"},25277:(e,a,t)=>{t.d(a,{Z:()=>n});const n=t.p+"assets/images/modeling-a01197b64e2922b2fe2218d98e97e0cb.png"},56225:(e,a,t)=>{t.d(a,{Z:()=>n});const n=t.p+"assets/images/pd_columns-487b4426459742c541d9d69564a89ddf.png"},85704:(e,a,t)=>{t.d(a,{Z:()=>n});const n=t.p+"assets/images/pd_describe-24dc50867458290a231b9f30de81f981.png"}}]);